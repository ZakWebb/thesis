%======================================================================
%   Zak Webb
%   Ph. D. Thesis
%   Department of Physics and Astronomy
%   University of Waterloo
% 
%   Mathematical Preliminaries
%======================================================================


\documentclass[../thesis-main/thesis-main]{subfiles}
\begin{document}

\chapter{Mathematical Preliminaries}
\label{chap:mathematical_preliminaries}


Several topics in this thesis require a background that not all researchers will have experience in.  As this is a physics thesis, here I review some basic complexity theory definitions used throughout this thesis.  Additionally, several lemmas used in this manuscript might be of independent interest, as their applicability is not restricted to the various models studied in this thesis. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Mathematical notation}
\label{sec:mathematical_notation}

Perhaps the most simple point that I would like to raise before the thesis begins in earnest is the notation that I will use throughout the paper.  Much of the paper uses notation not necessarily standard in every area of physics or computer science, and I want to make sure that no confusion occurs.  I will assume that various notations that are common do not need to be described, such at $\HHH$ describing a Hilbert space, or that $\II$ describes a the identity operator on a particular Hilbert space.

The first such notation will be for the shorthand definition of sets of particular size.  Namely,
\begin{equation}
[k] := \{1,2,\cdots, k\}.
\end{equation}
This is a set of size $k$, with the elements ordered and labeled by the integers from $1$ to $k$.  We will often think of these as elements from $\ZZ_{k}$, with addition and multiplication defined over the integers modulo $k$.

As we will also be working with graphs, we will want to note that the letter $G$ usually denotes a particular graph.  $V(G)$ then describes the vertex set of $G$, and $E(G)$ describes the edge set of $G$.  Note that this thesis will always deal with undirected graphs, with at most a single edge between vertices.  As such, the adjacency matrix (denoted $A(G)$) will be a symmetric $|V(G)|\times|V(G)|$ matrix, with entries given by 
\begin{align}
A(G)_{uv} = \begin{cases}
 1 & \{u,v\}\in E(G)\\
 0 & \text{otherwise}.
\end{cases}
\end{align}  Note that all entries of these adjacency matrices are either zero or one; this restriction on graphs forces them to be \emph{unweighted}, so that all edges are treated equally.  We might further restrict ourselves to graphs without any self-loops (diagonal entries in $A(G)$), which are called \emph{simple} graphs.

Much of the work in this thesis, especially when describing the ground energy of a particular Hamiltonian, deals only with positive semi-definite operators. If $A$ is a positive semi-definite matrix, then $\gamma(A)$ will denote the smallest non-zero eigenvalue of $A$.   Note that if $A$ has a 0-eigenvalue, then this corresponds to the energy gap between the ground state and the first excited state, but if $A$ does not have a 0-eigenvalue then this is simply the smallest eigenvalue of $A$.

In a related note, we will need to describe various operator norms used throughout this thesis.  Unless otherwise specified, for a given Hermitian operator $A$, we will denote $\Vert H \Vert$ by the largest (in absolute value) eigenvalue of $H$ (this is often called the spectral or infinity norm).  Additionally, $\Vert H\Vert_{\alpha}$ will denote the $\alpha$-norm of $H$, which is defined as
\begin{align}
  \Vert H \Vert_\alpha := \Big( \sum_{i\in [N]} |\lambda_i|^\alpha \Big)^{1/\alpha},
\end{align}
where $\{\lambda_i\}_{i\in[N]}$ are the eigenvalues of $H$.

Another notation to define is the restriction of an operator to a subspace.  Let us assume that $A$ acts on a Hilbert space $\HHH$, and that $\mathcal{S}$ is a subspace of $\HHH$.  We will then write the restriction of $A$ to the subspace $\mathcal{S}$ as $A\big|_\mathcal{S}$.  Note that this subspace might not be invariant under the action of $A$.


Often this paper will want to investigate systems with many particles, and we will want an operator to only act nontrivially on one particle.  If we have a Hilbert space $\HHH_{\text{total}} = \HHH_{\text{single}}^{\otimes N}$ that consists of $N$ copies of some single Hilbert space, and if we have an operator $M$ that acts on $\HHH_{\text{single}}$, we can define an operator $M^{(w)}$ that acts nontrivially only on the $w$-th copy of $\HHH_{\text{single}}$, namely
\begin{equation}
  M^{(w)} = \II^{\otimes w-1} \otimes M \otimes \II^{N- w}.
\end{equation}
In this manner, only the $w$-th copy of $\HHH_{\text{single}}$ is effected.

In a similar manner, if we have many particles living in identical Hilbert spaces, we might need to permute the particles for ease of notation or to define certain subspaces.  Letting $\HHH_{\text{total}} = \HHH_{\text{single}}^{\otimes N}$ consist of $N$ copies of a single Hilbert space, and letting $\pi \in S_N$ be a permutation the $N$ objects, we define the permutation operator $V_\pi$ acting on the basis states of $\HHH_{\text{total}}$ as
\begin{equation}
  V_\pi \ket{x_1,x_2,\cdots,x_N} = \ket{x_{\pi^{-1}(1)},x_{\pi^{-1}(2)},\cdots,x_{\pi^{-1}(N)}},
\end{equation}
and extend it linearly to the rest of the Hilbert space.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Quantum information}
%\label{sec:quantum_information}
%
%As this thesis is about quantum information, I will assume a familiarity with the basics 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Indistinguishable particles}
%\label{sec:indistinguishable_particles}
%
%A foundational aspect of this thesis is understanding how defining the small scale interactions between multiple particles effects the large scale behavior of the system.  As we want to define the large scale systems in terms of these small systems, the Hamiltonians that we will be interested in will have symmetries when we permute the different particles.
%
%More concretely, when we restrict our Hamiltonians to the case with $n$ particles, each particle will have the same (finite) Hilbert space to work with, and the Hamiltonian will be symmetric under a relabelling of the particles.  Explicitly, we will have that each particle will have a Hilbert space $\mathcal{H}_{\text{ind}} = \CC^{d}$ for some dimension $d$, so that the total Hilbert space of $n$ particles is $\mathcal{H}_{\text{tot}}  = \mathcal{H}_{\text{ind}}^{\otimes n} = \CC^{dn}$, and the Hamiltonian is an operator on $\mathcal{H}_{\text{tot}}$ that commutes with the operators that swap the individual particle Hilbert spaces.
%
%\todo{Make this more coherent}
%
%The reason that we do this is the simple fact that particles are almost always indistinguishable.  As such, any initial state for these multiparticle Hamiltonians will have the same symmetry as the underlying particles.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Complexity Theory}
\label{sec:complexity_theory}

While this thesis is for the physics department, many of the results require some basic quantum complexity theory.  In particular, the computer science idea for classification of computational problems in terms of the requisite resources gives a particularly nice interpretation of why certain physical systems don't equilibrate, and give a simple explanation on why certain systems do not have a known closed form solution.

This is a simple introduction, with a focus designed to make the rest of this thesis comprehensible to those without a background in complexity theory.  For a more formal introduction to Complexity Theory, I would recommend \cite{SipserToC}, with a more in depth review found in \cite{ABCC}.  For a focus on complexity as found in quantum information, I would recommend \cite{Wat09}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Languages and promise problems}

The main foundation of computational complexity is in the classification of languages based on the resources required to determine whether some given string is in a language.  Unfortunately, this requires the definition of many of these terms.  

In particular, what exactly is a string?  Any person who has taken a basic programming class knows that a string is simply a word, but the mathematical definition is slightly more complicated.  In particular, we first need to define an alphabet, and then define a string over a particular alphabet.  
\begin{definition}[Alphabet] An alphabet is a finite collection of symbols.
\end{definition}
Usually, an arbitrary alphabet is denoted by $\Gamma$, while the binary alphabet is denoted by $\Sigma = \{0,1\}$.  The chosen alphabet has no impact on a particular complexity result, as any finite alphabet can be represented via the binary alphabet with overhead that is logarithmic in the size of the original alphabet (essentially, just use a binary encoding of the new alphabet).  

With this definition of an alphabet, a string is simply a finite sequence of elements from the alphabet.  In particular, we define $\Gamma^n$ to be all length $n$ sequences of elements from $\Gamma$, and then define
\begin{equation}
  \Gamma^* = \bigcup_{n=0}^\infty \Gamma^n.
\end{equation}
With this, $\Gamma^*$ is the set of all strings over $\Gamma$.

Computational complexity then deals with understanding subsets of these strings.  In particular, let $\Pi_{\text{yes}}$ be a subset of $\Gamma^*$.  The language problem related to $\Pi_{\text{yes}}$ is to determine whether a given string $x\in \Gamma^*$ is contained within $\Pi_{\text{yes}}$ or not.  This can be trivial, such as for the case of $\Pi_{\text{yes}} = \Gamma^*$, or it can be impossible, such as in the case of the famous Halting Problem. 

Related to these language problems are promise problems, in which there are two subsets of $\Gamma^*$, namely $\Pi_{\text{yes}}$ and $\Pi_{\text{no}}$, such that $\Pi_{\text{yes}} \cap \Pi_{\text{no}} = \emptyset$.  We are then \emph{promised} that the $x\in \Gamma^*$ that we need to sort is contained either $\Pi_{\text{yes}} \cup \Pi_{\text{no}}$.  This generally opens up some more interesting problems, as without this restriction certain complexity classes do not make sense.

Most complexity classes related to quantum computing are classes of promise problems.

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Turing machines}



Up to this point, we have only discussed classifications of strings, and stated that we will want to understand the various resources required to sort a given string into one of two different sets, but we have not explained how these resources are defined.  There are various ways to do this, depending on the various computational model one is interested in, but to give the most intuition we will need to define a Turing Machine.  These machines are a mathematical construction that allow for the explicit definition of algorithms.  

At their most basic level, a Turing machine is simply a finite program along with a (countably) infinite tape that allows the machine to store information.  The input to the algorithm is initially written on the tape, and the machine starts in some initial configuration.  The machine can only access it's internal memory along with a single character at a time from the infinite tape, and the program progresses by changing the internal state of the machine, changing one character on the tape, and moving along the tape.  While extremely limited, these machines have so far captured our ideas of computation.

Formally, a Turing-machine is $M$ is described by a tuple $(\Gamma,Q,\delta)$, where $\Gamma$ is a finite set of symbol's that can be written on the infinite tape, $Q$ is a set of possible internal states that $M$ can store as internal memory, and $\delta$ is a function $Q\times \Gamma \rightarrow Q \times \Gamma\times \{L,S,R\}$ describing the required action of the machine $M$.  Included in $Q$ are two special ``halting'' states generally labeled \texttt{accept} and \texttt{reject}, such that the machine stops operating if it ever enters these two states, and the machine either accepts or rejects the current string.  Note that we always assume that the alphabet contains a special character \textvisiblespace {} that is not used for the input but denotes empty space along the infinite tape after the input string.

During an actual computation, a Turing Machine always starts with its internal state in a specified position, with the string used for input on the initial segment of the infinite tape and the special character \textvisiblespace{}  on every character after the input.  Additionally, the pointer of the machine is located at the beginning of the tape, so that the machine is able to start reading the input (if needed).  At each time step the machine then applies the transition function, updating its internal state, the character located at the current position of the tape, along with the current position of the tape until the machine reaches one of its halting states.

Note that there are several variations on these Turing Machines, such as those that have multiple infinite tapes instead of just one, and one that can move to an arbitrary position along the tape.  These variations do not change the overall computational power of the model, just make it slightly more efficient.  This definition is perhaps the most simple, and will suffice for now.


One slight modification that will be useful for us are machines that compute a particular function.  In particular, for a given function $f:\Gamma^*\rightarrow \Gamma^*$, we say that a Turing Machine $M$ computes the function $f$ if for all inputs $x$, the machine eventually halts and after it halts the tape will have $f(x)$ on the output tape (and nothing else).

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Resources}

With an explicit definition of Turing Machines, we also want to have some way to quantify the amount of resources used by a computation.  Since each machine is expected to work on strings of arbitrary length, we somehow need to quantify the resources in terms of the input to a given string.    So far, the important quantity in these resource problems has been the length of an input string $x$.  Basically, the number of characters has been the interesting aspect to measure, since any machine will at least need to read the string.

With this $n$ as the yardstick for any of our measurements, we then need to measure the length of the actual computation.  In general, there are two ways to measure this length: the number of transitions that the computation used before it halted (as a measure of time), or else the number of elements of the tape that the machine visited during its computation (as a measure of space).  It is important to realize that the exact value of these resources depend on the definitions used for the machine, such as the alphabet size or the number of internal states.  As such, we will generally not be interested in the exact value for a given input, but will be more interested in the asymptotic scaling of the resources.

These requisite resources will generally be something of the form $\OO(f(n))$ for some easily computable function $f$ such as a polynomial or an exponential in $n$.  These various scalings will give us a nice method of classifying the difficulty of computational problems.  In general, we will say that a specific Turing Machine $M$ runs in time $f(n)$ if for all inputs it halts in time $t(x)$ and $t(x) \in \OO(f(n))$.

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Uniform circuit families}

While Turing Machines are sufficient for classical computation, when we want to describe some quantum complexity classes it will be useful to instead discuss quantum circuits.  However, an important aspect of Turing Machines is that they are defined independently of the size of the input, while circuits need to have unique definitions for all different input sizes.

One might be tempted to simply define a computation via circuits by whether or not there exists a circuit of a given length, but this ends up giving an unreasonable amount of power to the computational model.  In particular, the algorithm can hide computation in the definition of the circuit, as opposed to the actual running of the circuit itself.  

To get around this, we will need to compute the circuit for the computation given the length.  Namely, we will have a Turing Machine take as input the string length in unary, and the machine will output a description of the circuit.  I won't go into the details here, but the 

\begin{definition}[Uniform family of circuits] A collection $\{C_x: x\in S\subseteq \Sigma^*\}$ of circuits is a (polynomial-time) \emph{uniform family of circuits} if there exists a deterministic Turing Machine $M$ such that 
\begin{itemize}
  \item M runs in polynomial time.
  \item For all $x\in S$, M outputs a description of $C_x$.
\end{itemize}
\end{definition}

Note that this definition makes no reference to the type of circuit, although we will generally assume that the circuit comes from some specific gate set.



%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Useful complexity classes}

Once we have an understanding of what defines a relation, and how these are related, we can attempt to classify those languages that require different resources in order to solve.

%%%
\subsubsection{Classical complexity classes}

  Perhaps the most well known question in computational complexity is the \PP{} vs \NP{} problem.  However, what exactly are these classes.  At a most basic level, one can think of \PP{} as those classification problems that have an efficient classical solution, while \NP{} are those that can be checked in an efficient manner.  


\begin{definition}[\PP]
  A promise problem $\AAA= (\AAA_{\text{yes}},\AAA_{\text{no}})$ is in the class \PP{} if there exists a polynomial-time Turing Machine $M$ such that 
 \begin{itemize}
   \item If $x\in \AAA_{\text{yes}}$, then $M(x)$ accepts.
   \item If $x\in \AAA_{\text{no}}$, then $M(x)$ rejects.
 \end{itemize}
\end{definition}

Note that the Turing Machine $M$ is required to halt on all inputs, and thus this is exactly what we mean by a polynomial-computation.  Some simple examples of languages in \PP{} are whether the triple $(a,b,c)$ satisfies $ab = c$, and whether a particular list of values is sorted.


\begin{definition}[\NP]
  A promise problem $\AAA=(\AAA_{\text{yes}},\AAA_{\text{no}})$ is in the class \NP if there exists a polynomial $q$ and a polynomial-time Turing Machine $M$ such that
  \begin{itemize}
    \item If $x\in \AAA_{\text{yes}}$, then there exists a string $y\in \Sigma^{q(|x|)}$ such that $M(x,y)$ accepts.
    \item If $x\in \AAA_{\text{no}}$, then for all strings $y\in \Sigma^{q(|x|)}$, $M(x,y)$ rejects.
  \end{itemize}
\end{definition}
Essentially, a language is in \NP{} if a given string can be proven to be in the language.  This includes useful problems such as whether a given graph has a 3-coloring, whether an integer $p$ has at least $k$ prime factors, and all problems in \PP.


%%%
\subsubsection{Bounded-Error Quantum Polynomial Time}

With these classical problems now defined, we will want to understand what happens when we include quantum mechanics.  There is a way to define a quantum Turing machine, in an analog to the classical case, but the current state of the art has instead gone toward using quantum circuits instead.  

Intuitively, the idea behind Bounded-Error Quantum Polynomial Time (\BQP) consists of those problems that can be solved by a quantum computer efficiently, and thus is the quantum version of \PP.  However, we need to somehow encode the circuit so that computation cannot be hidden in the circuit definition.  This is exactly the reason for our definition of uniform circuit families, as it is impossible to hide additional computation in a time-bounded Turing machine, especially when the quantum circuits themselves most likely have more computational power.

\begin{definition}[\BQP] A promise problem $\AAA = (\AAA_{\text{yes}},\AAA_{\text{no}})$ is in the class \BQP{} if there exist a uniform family of quantum circuits $Q = \{Q_n: n\in \NN\}$ such that 
  \begin{itemize}
    \item If $x\in \AAA_{\text{yes}}$, then $\AP(Q_{|x|},\ket{x}) \geq \frac{2}{3}$.
    \item If $x\in \AAA_{\text{no}}$, then $\AP(Q_{|x|},\ket{x}) \leq \frac{1}{3}$.
  \end{itemize}
\end{definition}

Note that $\AP(Q,\ket{\psi})$ is the acceptance probability of a circuit $Q$, with input $\ket{\psi}$, given by
\begin{align}
  \AP(Q,\ket{\psi})=\bra{\psi} Q^\dag (\ketbra{0}{0}\otimes \II) Q\ket{\psi}.
\end{align}
From this, we have that the class \BQP{} does not always output the correct answer, but it does with a bounded probability. 

If a problem is \BQP-hard, then we say that it is universal for quantum computing.

%%%
\subsubsection{Quantum Merlin-Arthur}

In addition to the efficient quantum computations, we also have those computations that are efficient to check with a quantum computer, \QMA.  This class is analogous to \NP{} in that it includes problems that are thought difficult for a quantum computer.  Intuitively, these problems are those for which an all powerful person gave you a ``proof'' state $\ket{\psi}$, and you could run this state through a quantum circuit and be convinced of some property.

\begin{definition}[\QMA]  A promise problem $\AAA = (\AAA_{\text{yes}},\AAA_{\text{no}})$ is in the class \QMA{} if there exists a uniform family of quantum circuits $Q=\{Q_n:n\in \NN\}$ such that
\begin{itemize}
  \item If $x\in \AAA_{\text{yes}}$, then there exists a state $\ket{\psi}\in \CC^{p(|x|)}$ such that $Q_{|x|}(\ket{x},\ket{\psi}) \geq \frac{2}{3}$.
  \item If $x\in \AAA_{\text{no}}$, then for all states $\ket{\psi}\in \CC^{p(|x|)}$, $Q_{|x|}(\ket{x},\ket{\psi}) \leq \frac{1}{3}$.
\end{itemize}
\end{definition}

Note that these constants $\frac{2}{3}$ and $\frac{1}{3}$ can actually be replaced by $1-2^{|x|}$ and $2^{|x|}$ if we would like.

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Reductions and Complete Problems}

While we are interested in these complexity classes, it is often difficult to work with the exact definitions used.  As an example, in the definition of \NP, to show something for all of the class we would somehow need to encode the entire computation of the Turing machine in our proof, which is a very difficult endeavour.  However, if we know of a particular problem such that every 

Essentially a reduction is a polynomial-time computable function from one computational problem to another.  Because this reduction is easy to compute, if we can solve the second problem, then we can also solve the first problem.  More concretely, let $\AAA = (\AAA_{\text{yes}},\AAA_{\text{no}}$ and $\BBB=(\BBB_{\text{yes}},\BBB_{\text{no}})$ be two promise problems.  We say that there is a reduction from $\AAA$ to $\BBB$ if there is an efficient function $f$ such that for each $a\in \AAA_{\text{yes}}$ $f(a)\in \BBB_{\text{yes}}$ and for each $a\in \AAA_{\text{no}}$, $f(a) \in \BBB_{\text{no}}$.  

With these reductions in mind, a particular problem $\AAA$ is hard for a given complexity class \class{C}{} if for every $\BBB\in$\class{C}, there is a reduction from $\BBB$ to $\AAA$.  In this way, we can think of $\AAA$ as being as hard as any problem in \class{C}{}.  A problem is \class{C}-complete if it is \class{C}-hard and contained in \class{C}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hamiltonian simulation}\label{sec:hamiltonian_simulation}

Several times in this thesis, we will need to show how to simulate the evolution of a sparse, row-computable Hamiltonian on a given state $\ket{\phi}$ using a quantum circuit.  The state $\ket{\phi}$ might be an efficiently computable state, or it might be provided to us in a \QMA-style procedure, but in either case we are really only interested in understanding the dynamics of the simulation.

The problem of simulating Hamiltonian dynamics has been featured rather heavily in the literature, as it was the original motivation that Feynman gave for quantum computers \cite{Fey82,Fey85}.  In particular, Lloyd showed how to simulate sums of local operators \cite{Llo96}, and this idea was generalized by Aharanov and Ta-Shma to (efficiently computable) sparse Hamiltonians \cite{AT03}.  Since then, various schemes have improved the requirements on time \cite{BACS07,WBHS11,BC12}, as well as the dependence on the precision \cite{BCCKS14,BCCKS15} and various other avenues of research \cite{Chi09, PQSV11}, have managed to greatly improve our ability to simulate quantum dynamics.  

While Hamiltonians that are a sum of local operations are relatively easy to understand, $d$-sparse Hamiltonians are relatively more complex.  The reason that much of the literature has focused on local Hamiltonians is that they are easy to specify, as we need only write down each of the local Hamiltonians.  In particular, these are succinct representations for Hamiltonians on an exponential-sized Hilbert space, such that each non-zero term of the Hamiltonian corresponding to a specific basis vector can be determined efficiently.  Additionally, these local-Hamiltonians are further restricted to only have non-zero transition amplitudes for states that satisfy some locality conditions, but for the purposes of simulation the succinctness property is what we care about.

Namely, the fact that a local Hamiltonian is succinctly representable is all that is used in the algorithms for simulating Hamiltonian dynamics.  As such, if we can generalize these properties, we can generalize the Hamiltonians that we can simulate.  A row-computable, $d$-spare matrix is such a generalization, in which each row of a given Hamiltonian has at most $d$ non-zero entries, and there exists some efficiently computable function $f_{i}(x)$ that outputs the value (and position) of the $i$th nonzero entry of the $x$th row.  Note that $k$-local Hamiltonians are $d$-sparse (for some $d$ depending on the local dimension and connectivity), and easily row-computable.

A recent simulation algorithms \cite{BCK15} uses several techniques, including quantum walk algorithms, simulations of linear combinations of unitaries, and Bessel functions, to simulate a given Hamiltonian, but their main result is the following
\begin{theorem}[Theorem 1 of \cite{BCK15}]\label{thm:hamiltonian_simulation}
A $d$-sparse Hamiltonian $H$ acting on $n$ qubits can be simulated for time $t$ within error $\epsilon$ with
\begin{align}
  \OO\Big( \tau \frac{ \log (\tau/\epsilon)}{\log\log (\tau/\epsilon)}\Big)
  \label{eq:hamiltonian_query_bound}
\end{align}
queries and 
\begin{align}
  \OO\Big( \tau \big[n + \log^{5/2} (\tau/\epsilon) \big] \frac{\log (\tau/\epsilon)}{\log\log(\tau/\epsilon)}\Big)
  \label{eq:hamiltonian_2qubit_bound}
\end{align}
additional $2$-qubit gates, where $\tau := d \Vert H \Vert t$.
\end{theorem}
Note that the theorem was proved in the black box model, where the function $f$ was provided via black box.  Assuming that $f$ is superlinear in both $n$ and $\log^{5/2}(\tau/\epsilon)$, the time-complexity for simulating such a Hamiltonian is simply the product of the complexity of $f$ with \eq{hamiltonian_query_bound}.  Note that if $f$ is efficient to compute, this is an efficient simulation of the Hamiltonian dynamics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Various Mathematical Lemmas}

In addition to these various mathematical definitions, it will also be useful to have a list of certain mathematical lemmas that will be used several times in the thesis.  These lemmas might also be of independent interest.
%%%%%%%%%%%%%%%%%
\subsection{Truncation Lemma}

Perhaps the first such lemma we called the truncation lemma.  The idea behind this lemma is to approximate the evolution of a state under some particular Hamiltonian with another, where the differences between the two Hamiltonians only occur far from the support of the given state.  One would expect that since the state must evolve ``far'' in order to reach the differences between the two Hamiltonians, the evolution between the two will be close.  This lemma makes this intuition precise.  This lemma was shown by Childs, Gosset, and Webb in \cite{MPQW}.

\begin{lemma}[Truncation Lemma]
\label{lem:truncation}
Let $H$ be a Hamiltonian acting on a Hilbert space $\mathcal{H}$ and let $\ket{\Phi}\in\mathcal{H}$ be a normalized state. Let
$\mathcal{K}$ be a subspace of $\mathcal{H}$, let $P$ be the projector onto $\mathcal{K}$,
and let $\tilde{H}=PHP$ be the Hamiltonian within this subspace. Suppose
that, for some $T>0$, $W\in\{H,\tilde{H}\}$, $N_0\in\NN$,
and $\delta>0$, we have, for all $0\leq t\leq T$, 
\begin{align}
e^{-iWt}|\Phi\rangle & = |\gamma(t)\rangle+|\epsilon(t)\rangle \text{ with }
\left\Vert |\epsilon(t)\rangle\right\Vert \leq \delta
\end{align}
and
\begin{align}
  (1-P) H^{r}|\gamma(t)\rangle & = 0 \text{ for all } r\in\NN \text{ satisfying} r < N_0.
\end{align}
Then, for all $0\leq t \leq T$, 
\begin{align}
  \Norm{\left(e^{-iHt}-e^{-i\tilde{H}t}\right)|\Phi\rangle}
  \leq \left(\frac{4e\norm{H}t}{N_0} + 2 \right) 
        \left(\delta + 2^{-N_0}(1+\delta)\right).
\end{align}
\end{lemma}

The basic idea as that $\ket{\gamma(t)}$ well approximates the evolution of $\ket{\Phi}$ under one of $H$ or $\tilde{H}$, and thus also gives good approximation to the evolution according to the other Hamiltonian. 

The proof of this lemma actually makes use of two different lemmas, one pertaining to the difference in evolution according to the Hamiltonians when the support of a given state is far from the differences, and one that bounds the accumulated error in a sequence of similar unitaries.

The first lemma assumes some locality condition on the Hamiltonian, and uses a Taylor series to bound the error in approximating the evolution according to the truncation.
\begin{proposition}
\label{prop:trunc_prop}Let $H$ be a Hamiltonian acting on a Hilbert
space $\mathcal{H}$, and let $\ket{\Phi}\in\mathcal{H}$ be a normalized state. Let
$\mathcal{K}$ be a subspace of $\mathcal{H}$ such that there exists an $N_0\in\NN$
so that for all $\ket{\alpha}\in \mathcal{K}^{\perp}$ and for all $n\in\NN$ with $n< N_0$, $\bra{\alpha}H^{n}\ket{\Phi}=0$.
Let $P$ be the projector onto $\mathcal{K}$ and let $\tilde{H}=PHP$ be
the Hamiltonian within this subspace.  Then
\begin{align}
\norm{e^{-it\tilde{H}}\ket{\Phi}-e^{-itH}\ket{\Phi}} \leq 2\left(\frac{e\norm{H}t}{N_0}\right)^{N_0}.
\end{align}
\end{proposition}

\begin{proof}
Define $\ket{\Phi(t)}$ and $\ket{\tilde\Phi(t)}$
as
\begin{align}
\ket{\Phi(t)}=e^{-itH}\ket{\Phi}=\sum_{k=0}^{\infty}\frac{(-it)^{k}}{k!}H^{k}\ket{\Phi}\qquad\ket{\tilde\Phi(t)}=e^{-it\tilde{H}}\ket{\Phi}=\sum_{k=0}^{\infty}\frac{(-it)^{k}}{k!}\tilde{H}^{k}\ket{\Phi}.
\end{align}
Note that by assumption, $\tilde{H}^{k}\ket{\Phi}=H^{k}\ket{\Phi}$ for all $k< N_0$, and thus the first $N_0$ terms in the two above sums are equal. Looking at the difference between these two states, we have
\begin{align}
\norm{\ket{\Phi(t)}-\ket{\tilde\Phi(t)}} & =\Norm{\sum_{k=0}^{\infty}\frac{(-it)^{k}}{k!}\left(H^{k}-\tilde{H}^{k}\right)\ket{\Phi}}\\
 & =\Norm{\sum_{k=0}^{N_0-1}\frac{(-it)^{k}}{k!}\left(H^{k}-\tilde{H}^{k}\right)\ket{\Phi}-\sum_{k=N_0}^{\infty}\frac{(-it)^{k}}{k!}\left(H^{k}-\tilde{H}^{k}\right)\ket{\Phi}}\\
 & \leq\sum_{k=N_0}^{\infty}\frac{t^{k}}{k!}\left(\norm{H}^{k}+\norm{\tilde{H}}^{k}\right) \\
 & \leq 2\sum_{k=N_0}^{\infty}\frac{t^{k}}{k!} \norm{H}^{k}
\end{align}
where the last step uses the fact that $\norm{\tilde{H}}\leq\norm{P}\norm{H}\norm{P} = \norm{H}$.  Thus for any $c \ge 1$, we have
\begin{align}
\norm{\ket{\Phi(t)}-\ket{\tilde\Phi(t)}}
 & \leq\frac{2}{c^{N_0}}\sum_{k=N_0}^{\infty}\frac{(ct)^{k}}{k!}\norm{H}^{k}\\
 & \leq\frac{2}{c^{N_0}}\exp(ct\norm{H}).
\end{align}
We obtain the best bound by choosing $c=N_0/\norm{Ht}$, which gives
\begin{align}
  \norm{\ket{\Phi(t)}-\ket{\tilde\Phi(t)}}
  \le 2\left(\frac{e\norm{H}t}{N_0}\right)^{N_0}
\end{align}
as claimed.  (If $c < 1$ then the bound is trivial.)
\end{proof}

The second proof is related to the difference between two different products of unitaries.

\begin{proposition}
\label{prop:hybrid}Let $U_1,\ldots, U_n$ and $V_1,\ldots, V_n$  be unitary operators.  Then for any $\ket{\psi}$,
\begin{equation}
  \Norm{\left(\prod_{i=n}^1 U_i - \prod_{i=n}^1 V_i \right) \ket{\psi}}   \le \sum_{j=1}^n \Bigg\|{(U_j - V_j)\prod_{i=j-1}^1 U_i \ket{\psi}}\Bigg\|.
\end{equation}
\end{proposition}

\begin{proof}
The proof is by induction on $n$.  The case $n=1$ is obvious.  For the induction step, we have \begin{align}   \Norm{\left(\prod_{i=n}^1 U_i - \prod_{i=n}^1 V_i \right) \ket{\psi}}   &= \Norm{\left(\prod_{i=n}^1 U_i - V_n \prod_{i=n-1}^1 U_i             + V_n \prod_{i=n-1}^1 U_i - \prod_{i=n}^1 V_i \right) \ket{\psi}} \\   &\le \Norm{(U_n - V_n) \prod_{i=n-1}^1 U_i \ket{\psi}}       +\Norm{\left(\prod_{i=n-1}^1 U_i - \prod_{i=n-1}^1 V_i \right) \ket{\psi}} \\   &\le \sum_{j=1}^n \Norm{(U_j - V_j)\prod_{i=j-1}^1 U_i \ket{\psi}} \end{align} where the last step uses the induction hypothesis. 
\end{proof}

\begin{proof}[Proof of {\lem{truncation}}]
For $M \in \NN$ write
\begin{align}
  \norm{(e^{-iHt}-e^{-i\tilde{H}t}) \ket{\Phi}}
  &= \Norm{\left(\left(e^{-iH\frac{t}{M}}\right)^{M} -
     \left(e^{-i\tilde{H}\frac{t}{M}}\right)^{M}\right) \ket{\Phi}} \\
  & \leq \sum_{j=1}^{M} \Norm{\left(e^{-iH\frac{t}{M}} - 
     e^{-i\tilde{H}\frac{t}{M}}\right) e^{-iW\left(j-1\right)\frac{t}{M}}
     \ket{\Phi}} \\
  & \leq \sum_{j=1}^{M} 
    \Norm{\left(e^{-iH\frac{t}{M}} - e^{-i\tilde{H}\frac{t}{M}}\right)
    \left(\ket{\gamma(\tfrac{(j-1)t}{M})} +
          \ket{\epsilon(\tfrac{(j-1)t}{M})}\right)} \\
  & \leq 2M\delta+\sum_{j=1}^{M} 
    \Norm{ \left(e^{-iH\frac{t}{M}} - e^{-i\tilde{H}\frac{t}{M}}\right)
    \frac{\ket{\gamma(\tfrac{(j-1)t}{M})}} 
         {\Norm{\ket{\gamma(\tfrac{(j-1)t}{M})}}}}
    \Norm{\ket{\gamma(\tfrac{(j-1)t}{M})}}\\ 
  & \leq 2M\delta 
    + 2M\left(\frac{e\norm{H}t}{M N_0}\right)^{N_0}(1+\delta)
\end{align}
where in the second line we have used \propo{hybrid} and in the last step we have used \propo{trunc_prop} and the fact that $\Vert |\gamma(t)\rangle\Vert \leq 1+\delta$.
Now, for some $\eta>1$, choose
\begin{align}
  M= \left\lceil \frac{\eta e\norm{H}t}{N_0} \right\rceil
\end{align}
for $0<t\leq T$ to get
\begin{align}
  \norm{(e^{-iHt}-e^{-i\tilde{H}t}) \ket{\Phi}}
  &\leq 2M\left(\delta + \eta^{-N_0}(1+\delta)\right) \\
  &\leq 2\left(\frac{\eta e\norm{H}t}{N_0} + 1 \right) 
        \left(\delta + \eta^{-N_0}(1+\delta)\right).
\end{align}
The choice $\eta=2$ gives the stated conclusion.
\end{proof}

Note that it would be slightly better to take a smaller value of $\eta$.  However, this does not significantly improve the final result; the above bound is simpler and sufficient for our purposes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Nullspace Projection Lemma}

When we discuss the ground spaces and ground energies of various Hamiltonians, we will often want to know what happens to the ground spaces and ground energies when two such Hamiltonians are added together (such as adding penalties enforcing particular initial states).  As such, the Nullspace Projection Lemma exactly discusses how such systems add together.  As far as I am aware this lemma was initially used (implicitly) by Mizel, et al., \cite{MLM99}.  We then used this in our proof of the \QMA-completeness for the Bose-Hubbard model \cite{BHQMA}.  We then found a similar lemma by Alicki, et al. \cite{AFH09}.  While the improvement is minor, we state and prove this better lemma:
\begin{lemma}[Nullspace Projection Lemma]
Let $H_A$ and $H_B$ be positive semi-definite matrices.  Suppose that the nullspace, $S$, of $H_A$ is nonempty, and that 
\begin{equation}
  \gamma\big(H_B|_S\big) \geq c > 0 \qquad \text{and} \qquad \gamma(H_A) \geq d > 0.
\end{equation}
Then,
\begin{equation}
  \gamma(H_A + H_B) \geq \frac{c d}{d + \norm{H_B}} .
\end{equation}
\label{lem:NPL}
\end{lemma}
\begin{proof}
Let $|\psi\rangle$ be a normalized state satisfying 
\begin{equation}
\langle\psi|H_{A}+H_{B}|\psi\rangle=\gamma(H_{A}+H_{B}).
\end{equation}
Let $\Pi_{S}$ be the projector onto the nullspace of $H_{A}$. First suppose that $\Pi_{S}|\psi\rangle=0$, in which case 
\begin{equation}
\langle\psi|H_{A}+H_{B}|\psi\rangle\geq\langle\psi|H_{A}|\psi\rangle\geq\gamma(H_{A})
\end{equation}
and the result follows. On the other hand, if $\Pi_{S}|\psi\rangle\neq0$ then we can write 
\begin{equation}
|\psi\rangle=\alpha|a\rangle+\beta|a^{\perp}\rangle
\end{equation}
with $|\alpha|^{2}+|\beta|^{2}=1$, $\alpha\neq0$, and two normalized states $|a\rangle$ and $|a^{\perp}\rangle$ such that $|a\rangle\in S$ and $|a^{\perp}\rangle\in S^{\perp}$. (If $\beta=0$ then we may choose $|a^{\perp}\rangle$ to be an arbitrary state in $S^{\perp}$ but in the following we fix one specific choice for concreteness.) Note that any state $|\phi\rangle$ in the nullspace of $H_{A}+H_{B}$ satisfies $H_{A}|\phi\rangle=0$ and hence $\langle\phi|a^{\perp}\rangle=0$. Since $\langle\phi|\psi\rangle=0$ and $\alpha\neq0$ we also see that $\langle\phi|a\rangle=0$. Hence any state
\begin{equation}
|f(q,r)\rangle=q|a\rangle+r|a^{\perp}\rangle
\end{equation}
is orthogonal to the nullspace of $H_{A}+H_{B}$, and
\begin{equation}
\gamma(H_{A}+H_{B})=\min_{|q|^{2}+|r|^{2}=1}\langle f(q,r)|H_{A}+H_{B}|f(q,r)\rangle.
\end{equation}

Within the subspace $Q$ spanned by $\ket{a}$ and $\ket{a^\perp}$, note that
\begin{equation}
  H_A|_Q = \begin{pmatrix} w & v^*\\
    v & z\end{pmatrix} \qquad H_B|_Q = \begin{pmatrix} 0 & 0\\
    0 & y \end{pmatrix}
\end{equation}
where $w=\langle a|H_{B}|a\rangle$, $v=\langle a^{\perp}|H_{B}|a\rangle$, $y=\langle a^{\perp}|H_{A}|a^{\perp}\rangle$, and $z=\langle a^{\perp}|H_{B}|a^{\perp}\rangle$, and that we are interested in the smaller eigenvalue of 
\begin{equation}
M = H_A|_Q + H_B|_Q =  \begin{pmatrix}
w & v^{*}\\
v & y+z
\end{pmatrix}.
\end{equation}
Letting $\epsilon_+$ and $\epsilon_-$ be the two eigenvalues of $M$ with $\epsilon_+\geq \epsilon_-$, note that 
\begin{equation}
  \epsilon_+ = \norm{M} \leq \norm{H_A|_Q} + \norm{H_B|_Q} \leq y+ \norm{H_B|_Q} \leq y + \norm{H_B},
\end{equation}
where we have used the Cauchy interlacing theorem to note that $\norm{H_B|_Q} \leq \norm{H_B}$.
Additionally, we have that
\begin{equation}
  \epsilon_+ \epsilon_- = \det (M) = w (y+z) - |v|^2 \geq wy
\end{equation}
where we used the fact that $H_B|_Q$ is positive-semidefinite.  Putting this together, we have that
\begin{equation}
 \gamma(H_A + H_B) = \min_{|q|^{2}+|r|^{2}=1}\langle f(q,r)|H_{A}+H_{B}|f(q,r)\rangle = \epsilon_- \geq \frac{w y}{y + \norm{H_B}}.
\end{equation}
As the right hand side increased monotonically with both $w$ and $y$, and as $w \geq \gamma(H_B|_S) \geq c$ and $y \geq \gamma(H_A) \geq d$, we have
\begin{equation}
  \gamma(H_A + H_B) \geq \frac{c d}{d + \norm{H_B}}
\end{equation}
as required.
\end{proof}


%%%%%%%%%%%%%%%%%
\biblio{}
\end{document}